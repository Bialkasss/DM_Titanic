THINGS THAT DO NOT MAKE ANY DIFFERENCE:
- null as new value
- everything is done on one set and at the end split it 
- making cross validation (?)

THINGS THAT INCREASE THE ACCURACY:
- discretization -> age and fare as discrete values ( k bins ) -> better performance :)
- changing order ( 2 attempt )


---
STARTING ACCURACY: 0.6974789915966386

----------------- changing age and Fare -------------------
1.1 attempt (accuracy 0.7871148459383753):
 - split to train and data set
 - null values as mean, mode
 - change categorical to descrite
 - normalization

1.2 attempt (accuracy 0.7843137254901961):
 - split to train and data set
 - null values as mean, mode
 - change categorical to descrite
 + assign Fare and Age to 3 classes
 - normalization

1.3 attempt (accuracy 0.7899159663865546):  !!!!!!!!! AT THIS STAGE THE BEST ACCURACY !!!!!!!!
 - split to train and data set
 - null values as mean, mode
 - change categorical to descrite
 + assign Fare and Age to 5 classes ( the same accuracy for 8 classes)
 - normalization


------------- null values as new values -------------------------
1.1 (accuracy 0.7843137254901961)
 - split to train and data set
 - null values as new
 - change categorical to descrite
 - assign Fare and Age to 5 classes ( the same accuracy for 8 classes)
 - normalization

1.2 (accuracy 0.7787114845938375)
 - split to train and data set
 - null values as new
 - change categorical to descrite
 - normalization

--------------- training on only one set and at the end split ---------
1. (accuracy 0.7843137254901961)
 - null values as new
 - change categorical to descrite
 ! assign Fare and Age to 5 classes changes nothing



--------------- cross validation ------------------------
    



----------- in 2 attempt in the feature selection(last step)------------------
- if from all components we choose only '0' ( we get 0.8207282913165266)
- if from all we choose '0' and '1' ( we get 0.7675070028011205)